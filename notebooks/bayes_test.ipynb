{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train two classifiers on MNIST\n",
    "\n",
    "Load and prepare MNIST data for training.\n",
    "\n",
    "We normalise all image values to the range [0, 1], and use a one-hot encoding for the lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Documents/miniconda3/envs/dml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "\n",
    "def load_data(input_size=28*28, output_size=10):\n",
    "    # load (downloaded if needed) the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "    # Reshape input data\n",
    "    x_train = x_train.reshape(x_train.shape[0], input_size).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], input_size).astype('float32')\n",
    "\n",
    "    #x_train = x_train.astype('float32')\n",
    "    #x_val = x_val.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # One-hot encode labels.\n",
    "    #y_train = keras.utils.to_categorical(y_train, output_size)\n",
    "    #y_test = keras.utils.to_categorical(y_test, output_size)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the two models, one for each of our two classifiers.\n",
    "\n",
    "One model is a multilayer perceptron, which we name 2NN, and the other is a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# create model: Ã  la 2NN\n",
    "def get_2nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "def get_cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(521, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, max_epochs=10, batch_size=50):\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_acc', patience=10, min_delta=0.0001)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, \n",
    "                       epochs=max_epochs, callbacks=[es], validation_data=(x_val, y_val))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def k_cross_val(get_model, x_train, y_train, x_test, y_test, k=5, epochs=1):\n",
    "    # One-hot encode labels.\n",
    "    y_train_hot = keras.utils.to_categorical(y_train, output_size)\n",
    "    y_test_hot = keras.utils.to_categorical(y_test, output_size)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    accs= np.zeros(k)\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(x_train, y_train)):\n",
    "        #print(\"Training on fold \" + str(index+1) + \"/5...\")\n",
    "\n",
    "        x, x_v = x_train[train_indices], x_train[val_indices]\n",
    "        y, y_v = y_train_hot[train_indices], y_train_hot[val_indices]\n",
    "\n",
    "        model = None\n",
    "        model = get_model()\n",
    "\n",
    "        history = train_model(model, x, y, x_v, y_v, epochs)\n",
    "        score = model.evaluate(x_test, y_test_hot, verbose=0)\n",
    "        accs[index] = score[1]\n",
    "        \n",
    "        print(f'Fold {index+1} Acc: {score[1]*100:.2f}%')\n",
    "    \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform m runs of k-fold cross validation to train both a 2NN classifier and a CNN classifier.\n",
    "We train the 2NN classifier for additional epochs to make it competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/5\n",
      "47995/47995 [==============================] - 2s 48us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.4212 - val_acc: 0.8836\n",
      "Epoch 2/5\n",
      "47995/47995 [==============================] - 2s 44us/step - loss: 0.3600 - acc: 0.9004 - val_loss: 0.3243 - val_acc: 0.9068\n",
      "Epoch 3/5\n",
      "47995/47995 [==============================] - 2s 46us/step - loss: 0.3000 - acc: 0.9148 - val_loss: 0.2869 - val_acc: 0.9159\n",
      "Epoch 4/5\n",
      "47995/47995 [==============================] - 2s 44us/step - loss: 0.2661 - acc: 0.9237 - val_loss: 0.2574 - val_acc: 0.9267\n",
      "Epoch 5/5\n",
      "47995/47995 [==============================] - 2s 46us/step - loss: 0.2400 - acc: 0.9307 - val_loss: 0.2332 - val_acc: 0.9326\n",
      "Fold 1 Acc: 93.43%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/5\n",
      "47998/47998 [==============================] - 2s 49us/step - loss: 0.8349 - acc: 0.7972 - val_loss: 0.4124 - val_acc: 0.8869\n",
      "Epoch 2/5\n",
      "47998/47998 [==============================] - 2s 42us/step - loss: 0.3544 - acc: 0.9004 - val_loss: 0.3226 - val_acc: 0.9092\n",
      "Epoch 3/5\n",
      "47998/47998 [==============================] - 2s 44us/step - loss: 0.2947 - acc: 0.9152 - val_loss: 0.2837 - val_acc: 0.9181\n",
      "Epoch 4/5\n",
      "47998/47998 [==============================] - 2s 44us/step - loss: 0.2606 - acc: 0.9250 - val_loss: 0.2593 - val_acc: 0.9262\n",
      "Epoch 5/5\n",
      "47998/47998 [==============================] - 2s 45us/step - loss: 0.2356 - acc: 0.9321 - val_loss: 0.2367 - val_acc: 0.9343\n",
      "Fold 2 Acc: 93.64%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.8271 - acc: 0.8007 - val_loss: 0.4104 - val_acc: 0.8887\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3547 - acc: 0.9005 - val_loss: 0.3233 - val_acc: 0.9069\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2951 - acc: 0.9165 - val_loss: 0.2796 - val_acc: 0.9206\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.2607 - acc: 0.9257 - val_loss: 0.2543 - val_acc: 0.9287\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2358 - acc: 0.9323 - val_loss: 0.2323 - val_acc: 0.9332\n",
      "Fold 3 Acc: 93.80%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/5\n",
      "48003/48003 [==============================] - 3s 57us/step - loss: 0.8195 - acc: 0.7991 - val_loss: 0.4251 - val_acc: 0.8791\n",
      "Epoch 2/5\n",
      "48003/48003 [==============================] - 2s 51us/step - loss: 0.3513 - acc: 0.9019 - val_loss: 0.3490 - val_acc: 0.8989\n",
      "Epoch 3/5\n",
      "48003/48003 [==============================] - 3s 54us/step - loss: 0.2899 - acc: 0.9179 - val_loss: 0.2965 - val_acc: 0.9154\n",
      "Epoch 4/5\n",
      "48003/48003 [==============================] - 2s 50us/step - loss: 0.2557 - acc: 0.9269 - val_loss: 0.2683 - val_acc: 0.9231\n",
      "Epoch 5/5\n",
      "48003/48003 [==============================] - 2s 51us/step - loss: 0.2312 - acc: 0.9336 - val_loss: 0.2456 - val_acc: 0.9285\n",
      "Fold 4 Acc: 93.75%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/5\n",
      "48004/48004 [==============================] - 3s 56us/step - loss: 0.8557 - acc: 0.7924 - val_loss: 0.4026 - val_acc: 0.8890\n",
      "Epoch 2/5\n",
      "48004/48004 [==============================] - 3s 54us/step - loss: 0.3620 - acc: 0.8977 - val_loss: 0.3385 - val_acc: 0.9006\n",
      "Epoch 3/5\n",
      "48004/48004 [==============================] - 3s 56us/step - loss: 0.3055 - acc: 0.9130 - val_loss: 0.3120 - val_acc: 0.9071\n",
      "Epoch 4/5\n",
      "48004/48004 [==============================] - 2s 50us/step - loss: 0.2729 - acc: 0.9210 - val_loss: 0.2524 - val_acc: 0.9251\n",
      "Epoch 5/5\n",
      "48004/48004 [==============================] - 3s 54us/step - loss: 0.2484 - acc: 0.9289 - val_loss: 0.2352 - val_acc: 0.9316\n",
      "Fold 5 Acc: 93.52%\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/5\n",
      "47995/47995 [==============================] - 3s 59us/step - loss: 0.8600 - acc: 0.7888 - val_loss: 0.4110 - val_acc: 0.8882\n",
      "Epoch 2/5\n",
      "47995/47995 [==============================] - 2s 51us/step - loss: 0.3611 - acc: 0.8992 - val_loss: 0.3197 - val_acc: 0.9084\n",
      "Epoch 3/5\n",
      "47995/47995 [==============================] - 3s 52us/step - loss: 0.2999 - acc: 0.9151 - val_loss: 0.2758 - val_acc: 0.9209\n",
      "Epoch 4/5\n",
      "47995/47995 [==============================] - 3s 52us/step - loss: 0.2647 - acc: 0.9241 - val_loss: 0.2507 - val_acc: 0.9273\n",
      "Epoch 5/5\n",
      "47995/47995 [==============================] - 2s 52us/step - loss: 0.2388 - acc: 0.9321 - val_loss: 0.2322 - val_acc: 0.9326\n",
      "Fold 1 Acc: 93.63%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/5\n",
      "47998/47998 [==============================] - 3s 55us/step - loss: 0.8529 - acc: 0.7836 - val_loss: 0.4241 - val_acc: 0.8809\n",
      "Epoch 2/5\n",
      "47998/47998 [==============================] - 3s 52us/step - loss: 0.3631 - acc: 0.8981 - val_loss: 0.3302 - val_acc: 0.9073\n",
      "Epoch 3/5\n",
      "47998/47998 [==============================] - 3s 53us/step - loss: 0.3040 - acc: 0.9136 - val_loss: 0.2879 - val_acc: 0.9167\n",
      "Epoch 4/5\n",
      "47998/47998 [==============================] - 2s 51us/step - loss: 0.2698 - acc: 0.9237 - val_loss: 0.2623 - val_acc: 0.9244\n",
      "Epoch 5/5\n",
      "47998/47998 [==============================] - 2s 51us/step - loss: 0.2446 - acc: 0.9303 - val_loss: 0.2400 - val_acc: 0.9304\n",
      "Fold 2 Acc: 93.48%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.8623 - acc: 0.7871 - val_loss: 0.4029 - val_acc: 0.8923\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.3511 - acc: 0.9021 - val_loss: 0.3118 - val_acc: 0.9137\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2900 - acc: 0.9179 - val_loss: 0.2745 - val_acc: 0.9233\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2563 - acc: 0.9266 - val_loss: 0.2498 - val_acc: 0.9301\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2320 - acc: 0.9332 - val_loss: 0.2310 - val_acc: 0.9343\n",
      "Fold 3 Acc: 93.79%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/5\n",
      "48003/48003 [==============================] - 3s 57us/step - loss: 0.8727 - acc: 0.7749 - val_loss: 0.4241 - val_acc: 0.8821\n",
      "Epoch 2/5\n",
      "48003/48003 [==============================] - 3s 53us/step - loss: 0.3653 - acc: 0.8970 - val_loss: 0.3238 - val_acc: 0.9085\n",
      "Epoch 3/5\n",
      "48003/48003 [==============================] - 2s 50us/step - loss: 0.3030 - acc: 0.9138 - val_loss: 0.3093 - val_acc: 0.9075\n",
      "Epoch 4/5\n",
      "48003/48003 [==============================] - 2s 50us/step - loss: 0.2664 - acc: 0.9232 - val_loss: 0.2553 - val_acc: 0.9296\n",
      "Epoch 5/5\n",
      "48003/48003 [==============================] - 2s 50us/step - loss: 0.2400 - acc: 0.9309 - val_loss: 0.2398 - val_acc: 0.9317\n",
      "Fold 4 Acc: 93.52%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/5\n",
      "48004/48004 [==============================] - 3s 57us/step - loss: 0.8370 - acc: 0.7958 - val_loss: 0.4145 - val_acc: 0.8849\n",
      "Epoch 2/5\n",
      "48004/48004 [==============================] - 2s 51us/step - loss: 0.3534 - acc: 0.9024 - val_loss: 0.3553 - val_acc: 0.8972\n",
      "Epoch 3/5\n",
      "48004/48004 [==============================] - 2s 51us/step - loss: 0.2980 - acc: 0.9150 - val_loss: 0.3096 - val_acc: 0.9071\n",
      "Epoch 4/5\n",
      "48004/48004 [==============================] - 3s 54us/step - loss: 0.2661 - acc: 0.9242 - val_loss: 0.2706 - val_acc: 0.9219\n",
      "Epoch 5/5\n",
      "48004/48004 [==============================] - 2s 51us/step - loss: 0.2420 - acc: 0.9309 - val_loss: 0.2516 - val_acc: 0.9272\n",
      "Fold 5 Acc: 93.36%\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/5\n",
      "47995/47995 [==============================] - 3s 57us/step - loss: 0.8415 - acc: 0.7902 - val_loss: 0.4165 - val_acc: 0.8866\n",
      "Epoch 2/5\n",
      "47995/47995 [==============================] - 3s 54us/step - loss: 0.3632 - acc: 0.8976 - val_loss: 0.3295 - val_acc: 0.9059\n",
      "Epoch 3/5\n",
      "47995/47995 [==============================] - 3s 54us/step - loss: 0.3053 - acc: 0.9119 - val_loss: 0.2909 - val_acc: 0.9163\n",
      "Epoch 4/5\n",
      "47995/47995 [==============================] - 3s 52us/step - loss: 0.2725 - acc: 0.9211 - val_loss: 0.2701 - val_acc: 0.9216\n",
      "Epoch 5/5\n",
      "47995/47995 [==============================] - 2s 48us/step - loss: 0.2484 - acc: 0.9293 - val_loss: 0.2488 - val_acc: 0.9270\n",
      "Fold 1 Acc: 93.24%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/5\n",
      "47998/47998 [==============================] - 3s 53us/step - loss: 0.8970 - acc: 0.7749 - val_loss: 0.4213 - val_acc: 0.8825\n",
      "Epoch 2/5\n",
      "47998/47998 [==============================] - 2s 52us/step - loss: 0.3717 - acc: 0.8953 - val_loss: 0.3277 - val_acc: 0.9039\n",
      "Epoch 3/5\n",
      "47998/47998 [==============================] - 2s 51us/step - loss: 0.3093 - acc: 0.9119 - val_loss: 0.2847 - val_acc: 0.9141\n",
      "Epoch 4/5\n",
      "47998/47998 [==============================] - 2s 51us/step - loss: 0.2750 - acc: 0.9211 - val_loss: 0.2600 - val_acc: 0.9236\n",
      "Epoch 5/5\n",
      "47998/47998 [==============================] - 2s 50us/step - loss: 0.2494 - acc: 0.9289 - val_loss: 0.2399 - val_acc: 0.9305\n",
      "Fold 2 Acc: 93.48%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.8419 - acc: 0.7936 - val_loss: 0.4161 - val_acc: 0.8865\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3531 - acc: 0.9011 - val_loss: 0.3279 - val_acc: 0.9042\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2941 - acc: 0.9172 - val_loss: 0.2932 - val_acc: 0.9151\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2616 - acc: 0.9262 - val_loss: 0.2645 - val_acc: 0.9238\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2372 - acc: 0.9326 - val_loss: 0.2432 - val_acc: 0.9294\n",
      "Fold 3 Acc: 93.64%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/5\n",
      "48003/48003 [==============================] - 3s 56us/step - loss: 0.8578 - acc: 0.7914 - val_loss: 0.4125 - val_acc: 0.8854\n",
      "Epoch 2/5\n",
      "48003/48003 [==============================] - 2s 50us/step - loss: 0.3570 - acc: 0.9000 - val_loss: 0.3718 - val_acc: 0.8924\n",
      "Epoch 3/5\n",
      "48003/48003 [==============================] - 2s 51us/step - loss: 0.2983 - acc: 0.9155 - val_loss: 0.2828 - val_acc: 0.9166\n",
      "Epoch 4/5\n",
      "48003/48003 [==============================] - 2s 49us/step - loss: 0.2651 - acc: 0.9246 - val_loss: 0.2592 - val_acc: 0.9221\n",
      "Epoch 5/5\n",
      "48003/48003 [==============================] - 2s 49us/step - loss: 0.2408 - acc: 0.9307 - val_loss: 0.2390 - val_acc: 0.9291\n",
      "Fold 4 Acc: 93.47%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/5\n",
      "48004/48004 [==============================] - 3s 58us/step - loss: 0.8281 - acc: 0.7938 - val_loss: 0.4137 - val_acc: 0.8874\n",
      "Epoch 2/5\n",
      "48004/48004 [==============================] - 2s 50us/step - loss: 0.3546 - acc: 0.9010 - val_loss: 0.3336 - val_acc: 0.9061\n",
      "Epoch 3/5\n",
      "48004/48004 [==============================] - 2s 52us/step - loss: 0.2957 - acc: 0.9153 - val_loss: 0.2884 - val_acc: 0.9194\n",
      "Epoch 4/5\n",
      "48004/48004 [==============================] - 2s 52us/step - loss: 0.2627 - acc: 0.9254 - val_loss: 0.2630 - val_acc: 0.9265\n",
      "Epoch 5/5\n",
      "48004/48004 [==============================] - 2s 50us/step - loss: 0.2389 - acc: 0.9322 - val_loss: 0.2435 - val_acc: 0.9321\n",
      "Fold 5 Acc: 93.34%\n",
      "x_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/1\n",
      "47995/47995 [==============================] - 26s 536us/step - loss: 0.6310 - acc: 0.8222 - val_loss: 0.2499 - val_acc: 0.9252\n",
      "Fold 1 Acc: 93.55%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/1\n",
      "47998/47998 [==============================] - 27s 562us/step - loss: 0.6656 - acc: 0.8149 - val_loss: 0.2421 - val_acc: 0.9298\n",
      "Fold 2 Acc: 93.33%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 27s 564us/step - loss: 0.6512 - acc: 0.8142 - val_loss: 0.2359 - val_acc: 0.9263\n",
      "Fold 3 Acc: 93.46%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/1\n",
      "48003/48003 [==============================] - 28s 573us/step - loss: 0.6537 - acc: 0.8148 - val_loss: 0.2356 - val_acc: 0.9297\n",
      "Fold 4 Acc: 93.49%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/1\n",
      "48004/48004 [==============================] - 27s 564us/step - loss: 0.6204 - acc: 0.8221 - val_loss: 0.2238 - val_acc: 0.9311\n",
      "Fold 5 Acc: 93.55%\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/1\n",
      "47995/47995 [==============================] - 28s 580us/step - loss: 0.6314 - acc: 0.8189 - val_loss: 0.2283 - val_acc: 0.9315\n",
      "Fold 1 Acc: 93.91%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/1\n",
      "47998/47998 [==============================] - 26s 547us/step - loss: 0.6420 - acc: 0.8162 - val_loss: 0.2301 - val_acc: 0.9296\n",
      "Fold 2 Acc: 93.54%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 28s 575us/step - loss: 0.6858 - acc: 0.8038 - val_loss: 0.2435 - val_acc: 0.9292\n",
      "Fold 3 Acc: 93.43%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/1\n",
      "48003/48003 [==============================] - 27s 565us/step - loss: 0.6505 - acc: 0.8146 - val_loss: 0.2439 - val_acc: 0.9256\n",
      "Fold 4 Acc: 93.27%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/1\n",
      "48004/48004 [==============================] - 28s 573us/step - loss: 0.6418 - acc: 0.8235 - val_loss: 0.2238 - val_acc: 0.9356\n",
      "Fold 5 Acc: 93.58%\n",
      "Train on 47995 samples, validate on 12005 samples\n",
      "Epoch 1/1\n",
      "47995/47995 [==============================] - 28s 574us/step - loss: 0.6463 - acc: 0.8134 - val_loss: 0.2261 - val_acc: 0.9321\n",
      "Fold 1 Acc: 93.65%\n",
      "Train on 47998 samples, validate on 12002 samples\n",
      "Epoch 1/1\n",
      "47998/47998 [==============================] - 28s 593us/step - loss: 0.6153 - acc: 0.8250 - val_loss: 0.2225 - val_acc: 0.9362\n",
      "Fold 2 Acc: 93.82%\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 28s 574us/step - loss: 0.6769 - acc: 0.8105 - val_loss: 0.2434 - val_acc: 0.9232\n",
      "Fold 3 Acc: 93.28%\n",
      "Train on 48003 samples, validate on 11997 samples\n",
      "Epoch 1/1\n",
      "48003/48003 [==============================] - 27s 567us/step - loss: 0.6450 - acc: 0.8123 - val_loss: 0.3638 - val_acc: 0.8890\n",
      "Fold 4 Acc: 88.97%\n",
      "Train on 48004 samples, validate on 11996 samples\n",
      "Epoch 1/1\n",
      "48004/48004 [==============================] - 28s 578us/step - loss: 0.6543 - acc: 0.8166 - val_loss: 0.4063 - val_acc: 0.8721\n",
      "Fold 5 Acc: 87.92%\n"
     ]
    }
   ],
   "source": [
    "m_runs = 3\n",
    "k = 5\n",
    "\n",
    "## Train a 2NN Classifier\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "accs_2NN = np.empty(0)\n",
    "for _ in range(m_runs):\n",
    "    res = k_cross_val(get_2nn_model, x_train, y_train, x_test, y_test, k, 5)\n",
    "    accs_2NN = np.append(accs_2NN, res)\n",
    "\n",
    "## Train a CNN Classifier\n",
    "# Reshape input data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "accs_CNN = np.empty(0)\n",
    "for _ in range(m_runs):\n",
    "    res = k_cross_val(get_cnn_model, x_train, y_train, x_test, y_test, k, 1)\n",
    "    accs_CNN = np.append(accs_CNN, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9343 0.9364 0.938  0.9375 0.9352 0.9363 0.9348 0.9379 0.9352 0.9336\n",
      " 0.9324 0.9348 0.9364 0.9347 0.9334]\n",
      "[0.9355 0.9333 0.9346 0.9349 0.9355 0.9391 0.9354 0.9343 0.9327 0.9358\n",
      " 0.9365 0.9382 0.9328 0.8897 0.8792]\n"
     ]
    }
   ],
   "source": [
    "print(accs_2NN)\n",
    "print(accs_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Correlated t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa86fbeaeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import baycomp\n",
    "%matplotlib inline\n",
    "\n",
    "names = (\"A\", \"B\") # A: 2NN, B: CNN\n",
    "probs, plot = baycomp.two_on_single(accs_2NN, accs_CNN, runs=m_runs, rope=0.01, plot=True, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.savefig('./posterior.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
